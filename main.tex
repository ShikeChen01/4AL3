\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

\usepackage{amsmath}      % math environments
\usepackage{amssymb}      % extra math symbols
\usepackage{amsfonts}     % math fonts, including \mathbb
\usepackage{bbm}          % gives \mathbbm{1} (indicator function)
\usepackage{graphicx}     % images
\usepackage{booktabs}     % pretty tables (optional)
\usepackage{hyperref}     % clickable refs (always useful)


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group X Progress Report:\\My Group's Project Name}


\author{First Author, Second Author, Third Author \\
  \texttt{\{xiaot13,loux8,chens356\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}
Currently majority of the financial market prediction models are relying 
on traditional index digging, technical analysis, regression models. 
However, with modern machine learning techniques emerging in NLP, 
the industry see a huge potential in bringing complex models to 
analyze and predict the market movement. 

Over the years, majority of the investors, including professionals, 
have struggled to consistently beat the S\&P500 index. In this challenge, 
we also understand the effect of the efficient market hypothesis, which 
states that stock prices fully reflect all available information. Thus, it is impossible to 
consistently achieve higher returns than average market returns on a
risk-adjusted basis, given that stock prices should only react to new information. 

However, with the advancement of machine learning techniques, if we are able 
to implement these new techniques to analyze the market data, we may be able to uncover
hidden patterns and profit. Since these are also considered as new information, the 
effective market hypothesis still holds. 

Since we do not know the preprocessing steps of the features, it is difficult for the 
team to use traditional, statistical methods to give meaningful insightful analysis. Thus, 
we decided to use predominantly unsuperised learning methods to analyze the data and
predict the S\&P500 return.

The overall objective of our project is to predict the S\&P500 return 
using the pre-processed data provided by Hull Tactical. 

\section{Related Work}

Predicting stock prices in an unsupervised manner is more naturally framed as a sequential
decision-making problem rather than a static supervised regression task. In this context,
reinforcement learning (RL), and in particular the algorithm Proximal Policy Optimisation
(PPO) \citep{Schulman2017}, offers a compelling framework. PPO is an on-policy actorâ€“critic
method that stabilises policy updates by clipping its surrogate objective, thereby limiting
destabilising policy shifts in non-stationary, noisy environments such as financial markets.

Adaptive trading strategies using deep RL have been applied to portfolio management. For example,
Huang et al.\ \citep{Huang2020} apply deep reinforcement learning to portfolio allocation,
including short-selling and arbitrage in continuous action spaces, showing that RL agents can
outperform benchmarks in equity markets. Jiang et al.\ \citep{Jiang2024} propose a model-free
DRL framework for portfolio selection under dynamic market complexity, demonstrating improved
performance in large-scale asset settings. On the risk-management front, Lam et al.\ \citep{Lam2023}
develop a unified risk-aware RL framework that uses coherent risk measures with non-linear
function approximation, delivering regret bounds in risk-constrained trading scenarios. Finally,
Soleymani \& Paquet \citep{Soleymani2021} introduce a graph-convolutional RL architecture (DeepPocket)
that models inter-asset correlations for portfolio optimisation and shows strong empirical performance.

Together, these studies underscore that combining PPO-style policy optimisation, temporal or graph
encoders (e.g., recurrent nets or GCNs), and risk-adjusted reward functions offers a promising
unsupervised framework for stock prediction and trading-policy development.



\section{Dataset}

The dataset used in this project is the same equity return prediction dataset  
referenced in the project proposal. An updated version of the dataset was  
released on November 6th. The update primarily expanded the number of  
observations and refined the calculation of forward returns. However, this  
revision did not materially change the statistical properties of the data  
distribution, nor did it require changes to our modeling pipeline, since our  
learning framework does not rely on absolute scale but rather on relative  
directional signals.

The dataset consist of 8 different feature groups. 
These feature groups are:
\begin{itemize}
  \item M* Market Dynamics/Technical features. 
  \item E* Macro Economic features.
  \item I* Interest Rate features.
  \item P* Price/Valuation features.
  \item V* Volatility features.
  \item S* Sentiment features. 
  \item MOM* Momentum features.
  \item D* Dummy/Binary features.
\end{itemize}
For example, a feature related to Price/Valueation will be denoted as column V1,
V2, etc.

Some analysis are ran on the dataset to understand the basic nature of the data.
\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{plots/DvsT.png}
  \caption{relationship between binary and target}
  \label{fig:target_hist}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{plots/top_corr.png}
  \caption{distribution of target variable}
  \label{fig:target_hist}
\end{figure}

From observing Figure 1 and Figure 2, as expected in financial forecasting problems, 
the empirical distribution of features and target is extremely noisy and return is distributed closely to zero. 
Features released are all have a weak to no visible relationship with the target variable. 
Therefore, when training, we normalize the target variable to have zero mean and unit variance which 
make sure the loss will not diminish during backward propagation.

The dataset also contains missing values, particularly in indicators  
constructed from rolling windows of different lengths. To avoid discarding  
information, we do not remove rows with missing data. Instead, we replace  
\texttt{NaN} values with zeros and add binary missing indicator flags for each  
affected feature:
\[
x^{(\text{filled})}_i =
\begin{cases}
0, & \text{if } x_i \text{ was missing} \\
x_i, & \text{otherwise}
\end{cases}
\]
This preserves potential information carried by the absence of values. Data  
loading and cleaning follows our \texttt{load\_data} and missing-indicator  
augmentation utility functions.

\vspace{6pt}
\section{Features}

We work directly with the numerical feature columns provided in the dataset.  
No manual feature engineering was performed beyond:
\begin{itemize}
    \item Standardization of all numeric features via $z$-score normalization.
    \item Addition of missing-value indicator features for all originally
          \texttt{NaN}-carrying columns.
    \item Optional inclusion of a constant bias term during reinforcement
          learning.
    \item Optional removal of values too small in magnitude to reduce noise.
\end{itemize}

Because financial features often exhibit collinearity, we conducted correlation  
inspections to assess which features carry directional information relative to  
\texttt{target}. However, no workable linear correlation found in this dataset. 
The processed data is feeded to an reinforcement learning environment to generate 
state frames, then the agent will learn based on the rewards from the 
reinforcement learning environment. 

\vspace{6pt}
\section{Implementation}

Our core modeling framework is a reinforcement learning (RL) agent based on  
Proximal Policy Optimization (PPO). Rather than predicting raw return values,  
the task is formulated as a sequential decision problem. At each timestep, the  
agent selects one of three discrete actions:
\[
a_t \in \{-1, 0, +1\}
\]
representing \texttt{sell}, \texttt{hold}, or \texttt{buy} decisions.

\subsection{Environment}

We define a custom environment that sequentially feeds feature vectors over  
time. The reward reflects directional correctness rather than magnitude:
Below are 2 different reward function implementation we experimented with:
\begin{itemize}
    \item \textbf{Simple directional reward:}
    \[
        r_t = a_t \cdot \Delta p_t
    \]
    where $a_t \in \{-1, +1\}$ is the action (sell / buy) and $\Delta p_t$ is the price change.
    \item \textbf{Risk-adjusted directional reward:}
    \[
        r_t = a_t \cdot \Delta p_t - \mathrm{sign}(x, \Delta p_t) 
    \]
    where $\mathrm{sign}(x, y)$ is defined as:
      \[
      \mathrm{sign}(x, y) =
      \begin{cases}
      \Lambda x, & \text{if } x \cdot y < 0,\\[6pt]
      0, & \text{otherwise.}
      \end{cases}
      \]
\end{itemize}

Currently we are still not satisfy with the reward function that we are currently using
since it does not consider Sharpe ratio, trading cost on switching sides, and other risk 
factors. We also want to encourage the model to make more sell decision rather than hold
or buy since the market is more tilted towards a bullish trend, and its ability to spot 
dips in the market is more valuable. We are actively working on
design and test different reward functions to improve both return and risk of model 
strategy. 

\subsection{Policy and Value Networks}
The agent maintains:
\begin{itemize}
    \item A policy network $\pi_\theta(a_t | s_t)$ producing action probabilities.
    \item A value network $V_\phi(s_t)$ estimating expected cumulative reward.
\end{itemize}

Both networks are multilayer perceptrons with ReLU activations. The policy is  
trained using the PPO clipped surrogate objective:
\[
L_{\pi} = -\mathbb{E}\left[\min(r_t(\theta)\hat{A}_t,
\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)\right]
\]
The value network is trained via mean-squared error against return-to-go.
In Future we will explore LSTM related architecture to enhance performance. 

\subsection{Training Procedure}

Training alternates between:
\begin{enumerate}
    \item Rollout: interact with the environment to collect trajectories.
    \item Update: compute generalized advantage estimates (GAE) and apply PPO.
\end{enumerate}

Although GPU acceleration is used for neural network forward and backward  
passes, rollout occurs step-by-step in Python. This creates a performance  
bottleneck where environment interaction becomes the dominant runtime cost.  
Parallel rollout environments are a planned optimization.

\subsection{Baselines}

To benchmark reinforcement learning performance, we also train three  
supervised models:
\begin{itemize}
    \item Gradient-Boosted Trees.
    \item Multilayer Perceptron Regression (MSE objective).
\end{itemize}

These baselines help evaluate whether reward-based directional optimization  
provides an advantage over direct regression on target. 

\section{Results and Evaluation}

As showm in Figure~\ref{fig:policy_loss},
\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{plots/policy_loss.png}
  \caption{PPO policy loss over training iterations}
  \label{fig:policy_loss}
\end{figure}
This is trained with batch size of 128, which is considered to be a small batch size.
In this setup, sign accuracy reach 53.93\% and MSE of 0.356282. 
The MSE is here is calculated against normalized target variable.
Finally, we observe the policy loss converge after 100 iterations. 

It is also observed with our latest experiment that 
with a large batch size 3000, the policy loss converge slower, with better results. 
However, we accidentally deleted the log file for that experiement therefore we can not 
show the plot here. 

Unfortunately, the training process of the best model is not documented. We are unable 
to retrain before the end of this report it because the time constraint and GPU resource limitation.

But we are still able to measure our best model performance against the baselines. 
\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{plots/best_return.png}
  \caption{Model performance comparison}
  \label{fig:performance_best}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{plots/baselines.png}
  \caption{Baseline model performance comparison}
  \label{fig:performance_base}
\end{figure}

Our baseline is two small MLPs regression and one gradient boosted tree model.

We understand one of the biggest drawback of implementing deep learning methods into 
financial market prediction is the inconsistency of the results and low explainability which
leads to high risk. We see our best model is outperforming the baselines by a large margin but, 
the performance is not consistant. Since the best model is ran on the same datapoint over a long period of time, 
although the model is relavantly small, we believe the finding may still not be valid. A good 
result would be a consistant outperformance over the expected return over all time periods with 
a relevantly small model. 

\section{Feedback and Plans}


\begin{itemize}
  \item Improve the reward function by incorporating risk-adjusted returns 
        (e.g., Sharpe ratio), trading costs, and additional risk controls.
  \item Experiment with different neural network architectures such as 
        p-sLSTM and GRU to improve agent performance.
  \item Implement parallel rollout environments to accelerate training.
  \item Complete the evaluation pipeline for more systematic analysis and visualization.
\end{itemize}

The TA's feedback emphasized the noisy nature of financial time series data 
and the inherent difficulty of price prediction. In response, we designed a 
careful preprocessing pipeline to stabilize the data and ensure that the model 
receives consistent and meaningful input features. We also revised our reward 
function to be more robust to noise and large adverse movements in price.
In future

\subsection{Citations}

\subsection{References}

\nocite{}

% \section*{Limitations}

\section*{Team Contributions}

All teammates are involved in training models and
contributed evenly to the project. There is no clear
division of labor since all members are capable
of doing all tasks, and we all did do the tasks to-
gether. All team members participated in weekly
brainstorm meetings, discussed ideas. All team
members contributed to algorithm deisgn, imple-
mentation, training, and evaluation.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
